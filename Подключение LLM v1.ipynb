{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a16b0515-fdc7-4c0c-a9d2-09b7c9eefb58",
   "metadata": {},
   "source": [
    "# Создание ИИ Агнета, который использует тестовые данные\n",
    "\n",
    "## План работ:\n",
    "- Подключить LLM модель\n",
    "- Обработать данные для того, чтобы предотсавить модели более понятную и структурированную информацию о платежах\n",
    "- Прогнать данные через ML модель (точность на тестовых данных сильно хромает)\n",
    "- Предоставить LLM модели промпт, в котором будут описаны параметры и данные, которые мы предоставляем модели, задача и пример вывода\n",
    "- Оценить полученные результаты\n",
    "\n",
    "## Цели работы:\n",
    "- Создать ИИ-агнета который будет определять степент риска транзакции по данным: назначение, время проведения, сумма транзакции и кто провел транзакцию\n",
    "- Точность должна быть от 90 процентов (но нет четкой метрики, по которой можно понять успешность агента)\n",
    "\n",
    "## Материалы по работе: \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6580a396-fa86-4633-9f28-bb99459c4d4f",
   "metadata": {},
   "source": [
    "### Подключение и настройка LLM\n",
    "\n",
    "Для данной работы используется модель `GigaChat`. Она была выбрана, потому что предоставляет достаточно бесплатных токенов для экспериментов и тестирования. Альтернативно можно использовать другие модели, например `Qwen`, в зависимости от требований к функционалу и объему доступных ресурсов.  \n",
    "\n",
    "Системный промпт хранится в файле `system_prompt.docx` в корневой папке проекта. В этом документе подробно описаны:  \n",
    "- задачи, которые должна выполнять модель;  \n",
    "- критерии оценки ответов модели и их приоритет;  \n",
    "- примеры входных данных и соответствующих выходных результатов.  \n",
    "\n",
    "Подключение системного промпта позволяет модели понимать контекст и задачи проекта, а также поддерживать единый стандарт качества ответов на протяжении всей сессии.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "48df1f55-0915-4525-b6fa-8e1c15ebe5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stanislav/Documents/Work/bank_agent/venv/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ngw.devices.sberbank.ru'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"access_token\":\"eyJjdHkiOiJqd3QiLCJlbmMiOiJBMjU2Q0JDLUhTNTEyIiwiYWxnIjoiUlNBLU9BRVAtMjU2In0.S3Inkfy6yeyDybiTqWojUX7grIJHE3GaZol1634DhgwzGuUSoCZezSq193kRSbweNH2oP4eL6T7P3FfSZd4_zqX6A8sqqXbV-uw1zMD-AM2zwmMpJ713jA55QeyfSSLS1IO7c5LoQj7qM9mZ26QF4ekp7TORKAghPMbEnfmxjPg0UYXze3bF2YV31aGDoUVr7k0rHQQHG81OjY1hNknxyjltXZNL7CD1ILsx9M2cfXpeT6YxZ03bXlQRAGKpvUEwFtfmMnYu-vAN7ofXXBZEnM1Hu7Bs9IML0wRB9Gi_IxSFoMBfIenPmNYYEHaaVyAghZ05sLUJmmEzsikHBiPUSA.4Fe1rHSA0RT1pFCajdY04A.h1x1vjLIMyVxYH5Ds2bpr2m8QuJzw73iZNIZY0EDdML6mRYrxFDt65Sk4icEUsM_vqvX9YVkNCNUZ-ZSkGhYAK0KJbstVs7eMF93xak3BUElf8AChQ7tQDzM3LlhaQPTTDDlzsVv99jVXVyeug8jGCczLM_dj6vPnFLV4tEPrmi9qVvAzzo9F39ATSS5rUjyNUaItGtesxoGdq4TJDUntzCYFGjWAXtXI9YJ_hZJljdq3kj7QynR2oi7v_UR01_Zt9BKYb1TEbMZWO-MO1dmgGw64LeGx6G38krrKzlOqL_cValkHpZCMQMQo3vO9WoURI-ebCd4hS6V1gyCwQbJ1mcQptDIJvpbkfDBURYiTMRdfIfyXGnXYC1GnuYDAC6yZRG04w2eVswLwTpNc-GC4tdFiS6-DD2VQ0AKQGsNzlTBHFipRN4TjujkbPd4eeKuixQTW2gQb8sSNLVYDgkDtbEE8R1cAbEd-wXpGAmntK--kjddICFuGyGIIjpoG0STakhDpXnz2u7AgyWUEnHjp-OFCSsMRztz-OmbycsVWpS1GOILaIJIQUf4lLOHnGuCXTTN3C5r2Fmq3YhB3ORJZVqcHVHrQZMtsK323oVzd1q6hA79w6EeJOIAVyvDODOBZDewQOpZ33piS5kZUpGQ2nNHm65cbkKxHD7X0BFbfqp1h-xoKngh41qVfp8XRsdmP5kxHTxGvRnXwLYdU-5tfMGlwU95E45K-9PXOYJNmtA.tpGPx2ucoGkhvTZ8rjpu0Hn7zjvlcvTIJHVZn2I51c0\",\"expires_at\":1760683241241}\n"
     ]
    }
   ],
   "source": [
    "# Подключаем LLM\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "url = \"https://ngw.devices.sberbank.ru:9443/api/v2/oauth\"\n",
    "\n",
    "payload={\n",
    "  'scope': 'GIGACHAT_API_PERS'\n",
    "}\n",
    "headers = {\n",
    "  'Content-Type': 'application/x-www-form-urlencoded',\n",
    "  'Accept': 'application/json',\n",
    "  'RqUID': '2aba969c-a22a-4816-a652-393a756a96c1',\n",
    "  'Authorization': 'Basic MDE5OWNlMTEtNmEyOC03OTBjLWJkOGMtZDdhYTI5OGI0MmZhOmRmOWYyOTg4LTVhMmEtNGZhNS04ZDdmLTNhMjE3ZDQ1ODY3Mg==',\n",
    "}\n",
    "\n",
    "response = requests.request(\"POST\", url, headers=headers, data=payload, verify=False)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a8b9ca53-65c9-4338-921c-c6ca27347ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = response.json()\n",
    "access_token = data['access_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "616f651e-754f-40a3-a783-3564e6e337eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ddgs import DDGS\n",
    "from langchain_gigachat.chat_models import GigaChat\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.tools import tool\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e819debb-2129-4344-882f-bafb878bce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from docx import Document\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "llm = GigaChat(credentials='MDE5OWNlMTEtNmEyOC03OTBjLWJkOGMtZDdhYTI5OGI0MmZhOmRmOWYyOTg4LTVhMmEtNGZhNS04ZDdmLTNhMjE3ZDQ1ODY3Mg==', model=\"GigaChat-2\", top_p=0, timeout=120, verify_ssl_certs=False,)\n",
    "\n",
    "doc_path = \"system_prompt.docx\"  \n",
    "doc = Document(doc_path)\n",
    "system_prompt = \"\\n\".join([p.text for p in doc.paragraphs if p.text.strip()])\n",
    "\n",
    "# Создаём память\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Добавляем системный промпт в память один раз\n",
    "memory.chat_memory.add_message(SystemMessage(content=system_prompt))\n",
    "\n",
    "# Передаём системный промпт один раз\n",
    "memory.chat_memory.add_message(SystemMessage(content=system_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8668f76e-42fd-4299-b20b-72894f79a124",
   "metadata": {},
   "source": [
    "### Создание цепочки действий для агента (Tools)\n",
    "\n",
    "В этом блоке мы определяем набор инструментов (`tool`) для агента. Каждый инструмент — это отдельная функция, выполняющая конкретный шаг обработки данных. Агент использует эти инструменты для решения задач, таких как анализ транзакций, выявление аномалий и подготовка данных для ML модели.  \n",
    "\n",
    "Использование инструментов позволяет:\n",
    "- структурировать процесс обработки данных,\n",
    "- повторно использовать код,\n",
    "- поддерживать прозрачность логики работы агента.\n",
    "\n",
    "#### Список инструментов:\n",
    "\n",
    "1. **`read_data_from_file`**  \n",
    "   Читает файл формата Excel или CSV и проверяет его на соответствие нужному формату.  \n",
    "   - Поддерживаемые форматы: `.csv`, `.xls`, `.xlsx`.\n",
    "   - Возвращает данные в формате JSON для последующей обработки.\n",
    "\n",
    "2. **`preprocess_data`**  \n",
    "   Выполняет все необходимые шаги предобработки данных для ML модели:\n",
    "   - добавляет столбцы с закодированными юридическими формами (`LabelEncoding`),\n",
    "   - бинарное кодирование признаков риска,\n",
    "   - подготовка финального набора колонок для модели.  \n",
    "   Возвращает подготовленные данные в JSON.\n",
    "\n",
    "3. **`local_model_analysis`**  \n",
    "   Применяет локальную ML модель (`RandomForestRegressor`) к подготовленным данным и добавляет столбец `ml_metric` — численный коэффициент риска.  \n",
    "   Используется для предоставления LLM дополнительной количественной оценки уровня риска.\n",
    "\n",
    "4. **`trace_money_flow`**  \n",
    "   Отслеживает движение средств между счетами:\n",
    "   - строит граф транзакций,\n",
    "   - выявляет цепочки переводов с ограничением по времени (`time_window_hours`) и глубине цепочки (`max_depth`).\n",
    "   - `добавить учет ООО ОАО и прочее`\n",
    "   \n",
    "   Возвращает JSON с найденными цепочками для анализа модели.\n",
    "5. **`detect_regular_payments`**  \n",
    "   Определяет регулярные платежи на основе:\n",
    "   - однотипности сумм,\n",
    "   - периодичности операций.  \n",
    "   Добавляет колонки `is_regular_payment`, `median_amount` и `median_interval_hours` в исходный набор данных.\n",
    "\n",
    "6. **`detect_anomalous_transactions`**  \n",
    "   Выявляет аномальные транзакции по признакам:\n",
    "   - сумма транзакции (`anomaly_amount`),\n",
    "   - периодичность операций (`anomaly_frequency`),\n",
    "   - назначение платежа (`anomaly_purpose`).  \n",
    "   Также формирует общий признак аномалии `anomaly_overall`.  \n",
    "   Результат возвращается в формате JSON для дальнейшего анализа.\n",
    "\n",
    "---\n",
    "\n",
    "**Примечание:**  \n",
    "Каждая функция принимает данные в формате JSON (список словарей), преобразует их в DataFrame для внутренней обработки, а затем возвращает результат снова в JSON. Это позволяет агенту легко передавать результаты между инструментами и сохранять совместимость с LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b237ccee-5b09-4256-9d29-38f656d929ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"read_data_from_file\", description=\"Читает файл excel/csv и проверяет его на соответствие нужному формату\")\n",
    "def read_data_from_file(file_path: str):\n",
    "    import os\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == \".csv\":\n",
    "        df = pd.read_csv(file_path)\n",
    "    elif ext in [\".xls\", \".xlsx\"]:\n",
    "        df = pd.read_excel(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Неподдерживаемый формат файла. Используйте CSV или Excel.\")\n",
    "\n",
    "    return df.to_json(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f2dc1502-5471-4847-84e1-2ef745574bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "\n",
    "def add_legal_form_columns(df, cols=[\"debit_name\", \"credit_name\"]):\n",
    "    '''\n",
    "        Функция для определния наименования организации\n",
    "        применяет LabelEncoding для кодирования и удобной передачи модели\n",
    "    '''\n",
    "    legal_forms = [\n",
    "        \"ООО\", \"ИП\", \"АО\", \"ПАО\", \"ЗАО\", \"ОАО\",\n",
    "        \"ГУП\", \"МУП\", \"ФГУП\", \"НКО\", \"АНО\", \"ТОО\", \"ОДО\", \"ЧУП\", \"КФХ\",\n",
    "        \"ПК\", \"ПТ\", \"СПК\", \"СНТ\", \"ТСЖ\", \"ЖСК\", \"ОП\", \"НП\", \"ФП\", \"Ассоциация\", \"Союз\"\n",
    "    ]\n",
    "    patterns = {form: re.compile(rf'\\b{form}\\b', re.IGNORECASE) for form in legal_forms}\n",
    "\n",
    "    def detect_legal_form(text):\n",
    "        cleaned = re.sub(r'[\\\"«»“”,.()*/]', ' ', str(text))\n",
    "        tokens = [t.upper() for t in re.split(r'\\s+', cleaned.strip()) if t]\n",
    "        for form, pattern in patterns.items():\n",
    "            if any(pattern.search(t) for t in tokens):\n",
    "                return form\n",
    "        return \"Другое\"\n",
    "\n",
    "    for col in cols:\n",
    "        new_col = f\"{col}_type\"\n",
    "        df[new_col] = df[col].astype(str).fillna(\"\").apply(detect_legal_form)\n",
    "        # LabelEncoding для колонки с типом\n",
    "        le = LabelEncoder()\n",
    "        df[f\"{new_col}_encoded\"] = le.fit_transform(df[new_col])\n",
    "    return df\n",
    "\n",
    "\n",
    "# Функция для бинарного кодирования риска (OneHot для ключевых слов)\n",
    "\n",
    "def onehot_risk_features(df):\n",
    "    '''\n",
    "        Функция для определения стоп-слов в назначении платежа\n",
    "        формирует колонки с помощью OneHotEcoder для удобной передачи модели\n",
    "    '''\n",
    "    df[\"purpose\"] = df[\"purpose\"].astype(str).fillna(\"\")\n",
    "    risk_keywords = {\n",
    "        \"loan_related\": [\"займ\", \"договор займа\", \"возврат займа\"],\n",
    "        \"unclear_transfer\": [\"взаиморасчёт\", \"перевод средств\", \"без договора\", \"перевод на карту\"],\n",
    "        \"no_contract\": [\"оплата без договора\", \"прочие расходы\", \"личные нужды\"],\n",
    "        \"crypto_activity\": [\"крипто\", \"биткоин\", \"usdt\", \"биржа\", \"coin\", \"crypto\"],\n",
    "        \"foreign_payment\": [\"иностранный перевод\", \"swift\", \"валютный счёт\", \"экспорт\"],\n",
    "        \"related_party_transfer\": [\"возврат займа\", \"займ физ. лицу\", \"передача активов\"],\n",
    "        \"cash_out\": [\"пополнение\", \"наличные\", \"выдача наличных\", \"обналичивание\"],\n",
    "        \"donation\": [\"благотворительность\", \"пожертвование\"],\n",
    "        \"agent_fee\": [\"агентское вознаграждение\", \"комиссионное\"],\n",
    "        \"service_payment\": [\"оплата услуг\", \"услуги по договору\", \"консультационные\", \"маркетинг\"],\n",
    "        \"bonus_related\": [\"премия\", \"бонус\", \"вознаграждение\"],\n",
    "        \"advance_payment\": [\"аванс\", \"предоплата\", \"частичная оплата\"],\n",
    "        \"lease_payment\": [\"аренда\", \"лизинг\", \"субаренда\"],\n",
    "        \"logistics\": [\"логистика\", \"транспорт\", \"перевозка\"],\n",
    "        \"has_docs\": [\"оплата по договору\", \"счёт-фактура\", \"акт выполненных работ\", \"ттн\"],\n",
    "        \"salary_related\": [\"зарплата\", \"заработная плата\", \"компенсация отпуска\", \"взносы\"],\n",
    "        \"utilities\": [\"коммунальные услуги\", \"аренда офиса\", \"интернет\", \"телефон\"],\n",
    "        \"tax_payment\": [\"налоги\", \"страховые взносы\", \"пфр\", \"фнс\", \"пенсионный фонд\"],\n",
    "        \"supplier_payment\": [\"поставщик\", \"поставка\", \"товары\", \"материалы\", \"сырьё\"],\n",
    "        \"internal_transfer\": [\"внутренний перевод\", \"между счетами организации\"]\n",
    "    }\n",
    "\n",
    "    for col_name, keywords in risk_keywords.items():\n",
    "        pattern = r'(' + '|'.join([re.escape(k.lower()) for k in keywords]) + r')'\n",
    "        df[col_name] = df[\"purpose\"].str.lower().str.contains(pattern, regex=True).astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Функция для LabelEncoding risk\n",
    "\n",
    "def encode_risk_label(df):\n",
    "    '''Кодирует risk'''\n",
    "    le_risk = LabelEncoder()\n",
    "    df['risk_encoded'] = le_risk.fit_transform(df['risk'].astype(str))\n",
    "    return df\n",
    "\n",
    "\n",
    "# Tool для агента\n",
    "\n",
    "@tool(\"preprocess_data\", description=\"Применяет все шаги предобработки для ML модели: добавление legal_form с LabelEncoding, бинарное кодирование риска и LabelEncoding risk\")\n",
    "def preprocess_data(json_records: str):\n",
    "    \n",
    "    records = json.loads(json_records)\n",
    "    # Преобразуем список словарей в DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "    # Применяем все функции\n",
    "    df = add_legal_form_columns(df)\n",
    "    df = onehot_risk_features(df)\n",
    "    df = encode_risk_label(df)\n",
    "    # Список нужных колонок\n",
    "    risk_cols = [\n",
    "        \"loan_related\", \"unclear_transfer\", \"no_contract\", \"crypto_activity\",\n",
    "        \"foreign_payment\", \"related_party_transfer\", \"cash_out\", \"donation\",\n",
    "        \"agent_fee\", \"service_payment\", \"bonus_related\", \"advance_payment\",\n",
    "        \"lease_payment\", \"logistics\", \"has_docs\", \"salary_related\", \"utilities\",\n",
    "        \"tax_payment\", \"supplier_payment\", \"internal_transfer\"\n",
    "    ]\n",
    "    final_cols = [\n",
    "        \"debit_name_type_encoded\",\n",
    "        \"credit_name_type_encoded\",\n",
    "        \"risk_encoded\", \"purpose\", \n",
    "        'anomaly_amount', 'anomaly_frequency',\n",
    "        'anomaly_purpose', 'anomaly_overall', 'is_regular_payment'\t\n",
    "    ] + risk_cols\n",
    "\n",
    "    df_final = df\n",
    "    return df_final.to_json(orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "89e83408-6b3a-43a7-8ee6-d20af529a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from langchain.tools import tool\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "@tool(\n",
    "    \"local_model_analysis\",\n",
    "    description=\"Применяет локальную ML модель и добавляет колонку ml_metric — численный коэффициент риска\"\n",
    ")\n",
    "def local_model_analysis(json_records: str):\n",
    "    \"\"\"\n",
    "    Применяет локальную модель RandomForestRegressor к списку записей\n",
    "    и добавляет к ним столбец 'ml_metric' — оценку степени риска.\n",
    "    \"\"\"\n",
    "    model_path = \"rf_reg_model.pkl\"\n",
    "    \n",
    "    # Преобразуем входные данные\n",
    "    records = json.loads(json_records)\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    # Загружаем модель\n",
    "    rf_model = joblib.load(model_path)\n",
    "\n",
    "    # Определяем набор признаков\n",
    "    risk_cols = [\n",
    "        \"loan_related\", \"unclear_transfer\", \"no_contract\", \"crypto_activity\",\n",
    "        \"foreign_payment\", \"related_party_transfer\", \"cash_out\", \"donation\",\n",
    "        \"agent_fee\", \"service_payment\", \"bonus_related\", \"advance_payment\",\n",
    "        \"lease_payment\", \"logistics\", \"has_docs\", \"salary_related\", \"utilities\",\n",
    "        \"tax_payment\", \"supplier_payment\", \"internal_transfer\"\n",
    "    ]\n",
    "    final_cols = [\n",
    "        \"debit_name_type_encoded\",\n",
    "        \"credit_name_type_encoded\",\n",
    "        \"risk_encoded\", \"purpose\"\n",
    "    ] + risk_cols\n",
    "\n",
    "    # Формируем фичи для модели (все колонки кроме purpose и risk_encoded)\n",
    "    feature_cols = [col for col in df.columns if col in final_cols and col not in [\"purpose\", \"risk_encoded\"]]\n",
    "\n",
    "    # Предсказание модели\n",
    "    y_pred = rf_model.predict(df[feature_cols])\n",
    "\n",
    "    # Добавляем столбец с метрикой\n",
    "    df[\"ml_metric\"] = [round(val, 2) for val in y_pred]\n",
    "\n",
    "    # Возвращаем обновлённые записи\n",
    "    return df.to_json(orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "943e768c-41b9-42a2-ae31-fcf89b10c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "\n",
    "@tool(\n",
    "    \"trace_money_flow\",\n",
    "    description=\"Отслеживает движение средств между счетами\"\n",
    ")\n",
    "def trace_money_flow(json_records:str):\n",
    "    \"\"\"\n",
    "    Отслеживает движение средств между счетами (INN),\n",
    "    строит граф и выявляет цепочки транзакций,\n",
    "    в которых переводы происходят с разрывом не более time_window_hours.\n",
    "\n",
    "    Аргументы:\n",
    "        df: DataFrame с колонками [date, debit_inn, credit_inn, debit_amount]\n",
    "        time_window_hours: максимально допустимое время между транзакциями в цепочке\n",
    "        max_depth: максимальная длина цепочки (по количеству шагов)\n",
    "        visualize: визуализировать итоговый граф\n",
    "\n",
    "    Возвращает  (старая версия):\n",
    "        chains_df — таблицу цепочек (для анализа/модели)\n",
    "        G — граф переводов (networkx.DiGraph)\n",
    "\n",
    "    Возвращает:\n",
    "        df_out — json с цепочками для анализа модели\n",
    "    \"\"\"\n",
    "    \n",
    "    time_window_hours=24 \n",
    "    max_depth=5\n",
    "    visualize=False\n",
    "    \n",
    "    # Подготовка данных\n",
    "    records = json.loads(json_records)\n",
    "    # Преобразуем список словарей в DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['debit_inn'] = df['debit_inn'].astype(str)\n",
    "    df['credit_inn'] = df['credit_inn'].astype(str)\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "    time_window = timedelta(hours=time_window_hours)\n",
    "    G = nx.DiGraph()\n",
    "    chains = []  # список найденных цепочек\n",
    "\n",
    "    # Добавляем все транзакции как рёбра в граф\n",
    "    for _, row in df.iterrows():\n",
    "        G.add_edge(\n",
    "            row['debit_inn'],\n",
    "            row['credit_inn'],\n",
    "            amount=row['debit_amount'],\n",
    "            date=row['date']\n",
    "        )\n",
    "\n",
    "    # Для ускорения создаём индекс по получателям\n",
    "    by_receiver = df.groupby('credit_inn')\n",
    "\n",
    "    # Теперь ищем хронологические цепочки\n",
    "    for _, tx in df.iterrows():\n",
    "        chain = [tx['debit_inn'], tx['credit_inn']]\n",
    "        last_time = tx['date']\n",
    "        current_node = tx['credit_inn']\n",
    "\n",
    "        # Рекурсивное расширение цепочки\n",
    "        depth = 1\n",
    "        while depth < max_depth and current_node in by_receiver.groups:\n",
    "            candidates = df[df['debit_inn'] == current_node]\n",
    "            candidates = candidates[candidates['date'] >= last_time]  # только позже\n",
    "            candidates = candidates[candidates['date'] - last_time <= time_window]  # в окне\n",
    "\n",
    "            if candidates.empty:\n",
    "                break\n",
    "\n",
    "            # Берём самую раннюю транзакцию после last_time\n",
    "            next_tx = candidates.sort_values('date').iloc[0]\n",
    "            chain.append(next_tx['credit_inn'])\n",
    "            last_time = next_tx['date']\n",
    "            current_node = next_tx['credit_inn']\n",
    "            depth += 1\n",
    "\n",
    "        if len(chain) > 2:\n",
    "            chains.append({\n",
    "                \"path\": \" → \".join(chain),\n",
    "                \"length\": len(chain),\n",
    "                \"start\": tx['date'],\n",
    "                \"end\": last_time,\n",
    "                \"duration_hours\": (last_time - tx['date']).total_seconds() / 3600.0,\n",
    "            })\n",
    "\n",
    "    chains_df = pd.DataFrame(chains).drop_duplicates(subset=[\"path\"])\n",
    "\n",
    "    # # Визуализация (убрать в финале)\n",
    "    # if visualize and len(G) > 0:\n",
    "    #     plt.figure(figsize=(10, 7))\n",
    "    #     pos = nx.spring_layout(G, k=0.5)\n",
    "    #     nx.draw(\n",
    "    #         G, pos,\n",
    "    #         with_labels=True,\n",
    "    #         node_color=\"lightblue\",\n",
    "    #         node_size=1200,\n",
    "    #         font_size=8,\n",
    "    #         arrows=True\n",
    "    #     )\n",
    "    #     edge_labels = {\n",
    "    #         (u, v): d['date'].strftime(\"%m-%d %H:%M\") for u, v, d in G.edges(data=True)\n",
    "    #     }\n",
    "    #     nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n",
    "    #     plt.title(f\"Движение средств (окно {time_window_hours}ч)\")\n",
    "    #     plt.show()\n",
    "\n",
    "    # return chains_df, G\n",
    "    df_out = chains_df.to_json(orient=\"records\")\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e04c4d94-4c81-47e2-b2c4-66615b48fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "@tool(\n",
    "    \"detect_regular_payments\",\n",
    "    description=\"Отслеживает регулярные операции и добавляет признаки регулярности в исходный датасет\"\n",
    ")\n",
    "def detect_regular_payments(json_records: str):\n",
    "    \"\"\"\n",
    "    Обнаруживает регулярные операции и помечает их в исходном наборе данных.\n",
    "\n",
    "    Ключевые аргументы:\n",
    "        json_records: JSON-строка списка словарей с колонками:\n",
    "            ['date', 'debit_inn', 'credit_inn', 'debit_amount', 'credit_amount']\n",
    "\n",
    "    Возвращает:\n",
    "        df_out: JSON исходного DataFrame с добавленными колонками:\n",
    "            - is_regular_payment (bool)\n",
    "            - median_amount (float)\n",
    "            - median_interval_hours (float)\n",
    "    \"\"\"\n",
    "    amount_tolerance = 0.3\n",
    "    min_occurrences = 3\n",
    "    frequency_tolerance_hours = 2\n",
    "\n",
    "    # Преобразуем входные данные в DataFrame\n",
    "    records = json.loads(json_records)\n",
    "    df = pd.DataFrame(records)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "    # Добавляем новые столбцы\n",
    "    df['is_regular_payment'] = False\n",
    "    df['median_amount'] = np.nan\n",
    "    df['median_interval_hours'] = np.nan\n",
    "\n",
    "    # Анализ регулярности по парам (отправитель, получатель)\n",
    "    for (credit_inn, debit_inn), group in df.groupby(['credit_inn', 'debit_inn']):\n",
    "        amounts = group['credit_amount'].values\n",
    "        dates = group['date'].values\n",
    "\n",
    "        if len(amounts) < min_occurrences:\n",
    "            continue\n",
    "\n",
    "        # Проверка на однотипность сумм\n",
    "        median_amount = np.median(amounts)\n",
    "        if np.all(np.abs(amounts - median_amount) / median_amount <= amount_tolerance):\n",
    "            # Проверка регулярности по времени\n",
    "            if len(dates) >= 2:\n",
    "                intervals = np.diff(dates).astype('timedelta64[s]').astype(float)\n",
    "                median_interval = np.median(intervals)\n",
    "                if np.all(np.abs(intervals - median_interval) <= frequency_tolerance_hours):\n",
    "                    # Помечаем все транзакции этой пары как регулярные\n",
    "                    mask = (df['credit_inn'] == credit_inn) & (df['debit_inn'] == debit_inn)\n",
    "                    df.loc[mask, 'is_regular_payment'] = True\n",
    "                    df.loc[mask, 'median_amount'] = median_amount\n",
    "                    df.loc[mask, 'median_interval_hours'] = median_interval\n",
    "\n",
    "    df_out = df.to_json(orient=\"records\", date_format=\"iso\")\n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "024cc905-f68a-455d-8d03-a872bd41647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\n",
    "    \"detect_anomalous_transactions\",\n",
    "    description=\"Выявляет аномальные транзакции для операций по кредиту по критериям: сумма транзакции, периодичность транзакций, назначение\"\n",
    ")\n",
    "def detect_anomalous_transactions(json_records:str):\n",
    "    \"\"\"\n",
    "    Определяет нетипичные транзакции для пользователей по сумме, периодичности и назначению платежа.\n",
    "    \n",
    "    Аргументы:\n",
    "        df: DataFrame с колонками [date, debit_inn, credit_inn, debit_amount, credit_amount] \n",
    "            и опционально purpose_col\n",
    "        user_col: колонка с пользователем (по кредиту или дебету)\n",
    "        amount_col: колонка с суммой для анализа (credit_amount или debit_amount)\n",
    "        date_col: колонка с датой транзакции\n",
    "        purpose_col: колонка с назначением платежа (опционально)\n",
    "        z_threshold: число стандартных отклонений для определения нетипичной суммы\n",
    "        \n",
    "    Возвращает:\n",
    "        DataFrame с добавленными колонками:\n",
    "            'anomaly_amount' - True, если сумма нетипичная\n",
    "            'anomaly_frequency' - True, если нарушена периодичность\n",
    "            'anomaly_purpose' - True, если назначение нетипичное\n",
    "            'anomaly_overall' - True, если любая из аномалий True\n",
    "    \"\"\"\n",
    "\n",
    "    user_col='credit_inn'\n",
    "    amount_col='credit_amount'\n",
    "    date_col='date' \n",
    "    purpose_col='puprose'\n",
    "    z_threshold=3\n",
    "\n",
    "    records = json.loads(json_records)\n",
    "    # Преобразуем список словарей в DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    df = df.sort_values([user_col, date_col])\n",
    "    \n",
    "    df['anomaly_amount'] = False\n",
    "    df['anomaly_frequency'] = False\n",
    "    df['anomaly_purpose'] = False\n",
    "    \n",
    "    # Группируем по пользователю\n",
    "    for user, group in df.groupby(user_col):\n",
    "        # --- Аномалия по сумме ---\n",
    "        amounts = group[amount_col]\n",
    "        mean = amounts.mean()\n",
    "        std = amounts.std()\n",
    "        df.loc[group.index, 'anomaly_amount'] = (np.abs(amounts - mean) > z_threshold * std)\n",
    "        \n",
    "        # --- Аномалия по периодичности ---\n",
    "        dates = group[date_col].sort_values()\n",
    "        if len(dates) > 1:\n",
    "            intervals = dates.diff().dt.total_seconds().iloc[1:] / 3600  # интервалы в часах\n",
    "            median_interval = intervals.median()\n",
    "            # Метка аномалии, если интервал отклоняется больше чем ±50% от медианного\n",
    "            anomaly_freq_mask = np.abs(intervals - median_interval) > 0.5 * median_interval\n",
    "            # Сдвигаем индексы на 1, т.к. diff() уменьшает длину на 1\n",
    "            df.loc[dates.index[1:], 'anomaly_frequency'] = anomaly_freq_mask.values\n",
    "        \n",
    "        # --- Аномалия по назначению платежа ---\n",
    "        if purpose_col and purpose_col in df.columns:\n",
    "            purpose_counts = group[purpose_col].value_counts()\n",
    "            rare_purposes = purpose_counts[purpose_counts == 1].index\n",
    "            df.loc[group.index, 'anomaly_purpose'] = group[purpose_col].isin(rare_purposes)\n",
    "    \n",
    "    # --- Общая аномалия ---\n",
    "    df['anomaly_overall'] = df[['anomaly_amount', 'anomaly_frequency', 'anomaly_purpose']].any(axis=1)\n",
    "    df_out = df.to_json(orient=\"records\")\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af187830-973c-4fc9-a27d-ccae93e1aac4",
   "metadata": {},
   "source": [
    "### Анализ транзакций с использованием агента и LLM\n",
    "\n",
    "В этом блоке выполняется полный процесс обработки и анализа транзакций с использованием подготовленных инструментов (`tools`) и языковой модели (`LLM`). Основные этапы и цели блока:\n",
    "\n",
    "1. **Инициализация агента и подключение инструментов**\n",
    "   - Создаётся агент, который объединяет все функции-инструменты для анализа данных.\n",
    "   - Агент может автоматически использовать эти инструменты для различных задач: чтение данных, предобработка, локальный ML-анализ, отслеживание движения средств, выявление регулярных и аномальных транзакций.\n",
    "\n",
    "2. **Подготовка данных**\n",
    "   - Загружается исходный CSV-файл с транзакциями.\n",
    "   - Выбирается случайная подвыборка для ускоренного анализа.\n",
    "   - Сохраняется новый CSV-файл с выбранными строками, который будет использоваться для пакетной обработки.\n",
    "\n",
    "3. **Последовательная обработка данных инструментами**\n",
    "   - Данные проходят несколько этапов:\n",
    "     - чтение и преобразование в JSON,\n",
    "     - выявление аномалий по сумме, частоте и назначению платежей,\n",
    "     - определение регулярных платежей,\n",
    "     - построение цепочек движения средств между счетами,\n",
    "     - подготовка данных для ML-модели,\n",
    "     - применение локальной модели для расчёта коэффициента риска.\n",
    "   - Каждый инструмент добавляет новые признаки и метки к данным, делая их готовыми для анализа LLM.\n",
    "\n",
    "4. **Очистка данных**\n",
    "   - Удаляются лишние поля, которые не нужны для анализа LLM.\n",
    "   - Это облегчает структуру данных и снижает вероятность ошибок при работе с моделью.\n",
    "\n",
    "5. **Безопасный парсинг JSON от LLM**\n",
    "   - Поскольку ответы модели могут содержать лишний текст или Markdown-разметку, используются функции `safe_parse_json` и `parse_llm_transactions`.\n",
    "   - Они корректно извлекают JSON из текста и преобразуют его в список словарей для дальнейшей обработки.\n",
    "\n",
    "6. **Пакетная обработка транзакций**\n",
    "   - Данные отправляются в LLM частями (`batch_size=10`) для уменьшения нагрузки.\n",
    "   - Для каждого пакета LLM анализирует транзакции и цепочки движения средств.\n",
    "   - Результаты собираются в единый список `results`.\n",
    "\n",
    "7. **Сохранение и оформление результатов**\n",
    "   - Результаты конвертируются в DataFrame и сохраняются в Excel.\n",
    "   - Настраивается автофильтр для удобного просмотра.\n",
    "   - Добавляется цветовая маркировка по уровню риска (`зелёный`, `жёлтый`, `красный`) для быстрой визуальной оценки.\n",
    "   - Итоговый файл готов к использованию для отчётов или дальнейшего анализа.\n",
    "\n",
    "**Ожидаемый результат блока:**\n",
    "- Полностью обработанный набор транзакций с дополнительными признаками (анализ риска, аномалии, регулярные платежи, цепочки движения средств).\n",
    "- Excel-файл с визуальной маркировкой рисков и возможностью фильтрации для удобного анализа.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b54848bc-ad75-4641-8465-e0345c71cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [read_data_from_file, preprocess_data, local_model_analysis, trace_money_flow, detect_regular_payments, detect_anomalous_transactions]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=\"chat-zero-shot-react-description\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2f995d46-1486-45ce-a90e-dde8537da0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохранено первые 5 строк в файл: data/statement_main_5.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Читаем исходный CSV\n",
    "df = pd.read_csv('data/statement_main.csv')\n",
    "\n",
    "# Берем случанйные 5 строк\n",
    "df_5 = df.sample(50)\n",
    "\n",
    "\n",
    "original_file = 'data/statement_main.csv'\n",
    "new_file = original_file.replace('.csv', '_5.csv')\n",
    "\n",
    "# Сохраняем новый CSV\n",
    "df_5.to_csv(new_file, index=False)\n",
    "\n",
    "print(f\"Сохранено первые 5 строк в файл: {new_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a805ca6c-415f-4424-bcd4-ea3bbb6e4eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hx/6vltr8z949358k44l1m6_b7m0000gn/T/ipykernel_1059/1750840877.py:70: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[col_name] = df[\"purpose\"].str.lower().str.contains(pattern, regex=True).astype(int)\n",
      "/var/folders/hx/6vltr8z949358k44l1m6_b7m0000gn/T/ipykernel_1059/1750840877.py:70: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[col_name] = df[\"purpose\"].str.lower().str.contains(pattern, regex=True).astype(int)\n",
      "/var/folders/hx/6vltr8z949358k44l1m6_b7m0000gn/T/ipykernel_1059/1750840877.py:70: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[col_name] = df[\"purpose\"].str.lower().str.contains(pattern, regex=True).astype(int)\n",
      "/var/folders/hx/6vltr8z949358k44l1m6_b7m0000gn/T/ipykernel_1059/1750840877.py:70: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[col_name] = df[\"purpose\"].str.lower().str.contains(pattern, regex=True).astype(int)\n",
      "/var/folders/hx/6vltr8z949358k44l1m6_b7m0000gn/T/ipykernel_1059/1750840877.py:70: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[col_name] = df[\"purpose\"].str.lower().str.contains(pattern, regex=True).astype(int)\n",
      "/var/folders/hx/6vltr8z949358k44l1m6_b7m0000gn/T/ipykernel_1059/1750840877.py:70: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[col_name] = df[\"purpose\"].str.lower().str.contains(pattern, regex=True).astype(int)\n",
      "/var/folders/hx/6vltr8z949358k44l1m6_b7m0000gn/T/ipykernel_1059/1750840877.py:70: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[col_name] = df[\"purpose\"].str.lower().str.contains(pattern, regex=True).astype(int)\n",
      "/var/folders/hx/6vltr8z949358k44l1m6_b7m0000gn/T/ipykernel_1059/1750840877.py:70: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[col_name] = df[\"purpose\"].str.lower().str.contains(pattern, regex=True).astype(int)\n",
      "/var/folders/hx/6vltr8z949358k44l1m6_b7m0000gn/T/ipykernel_1059/1750840877.py:70: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[col_name] = df[\"purpose\"].str.lower().str.contains(pattern, regex=True).astype(int)\n",
      "/var/folders/hx/6vltr8z949358k44l1m6_b7m0000gn/T/ipykernel_1059/1750840877.py:70: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[col_name] = df[\"purpose\"].str.lower().str.contains(pattern, regex=True).astype(int)\n",
      "/var/folders/hx/6vltr8z949358k44l1m6_b7m0000gn/T/ipykernel_1059/1750840877.py:70: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[col_name] = df[\"purpose\"].str.lower().str.contains(pattern, regex=True).astype(int)\n",
      "/var/folders/hx/6vltr8z949358k44l1m6_b7m0000gn/T/ipykernel_1059/1750840877.py:70: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[col_name] = df[\"purpose\"].str.lower().str.contains(pattern, regex=True).astype(int)\n",
      "/var/folders/hx/6vltr8z949358k44l1m6_b7m0000gn/T/ipykernel_1059/1750840877.py:70: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[col_name] = df[\"purpose\"].str.lower().str.contains(pattern, regex=True).astype(int)\n",
      "/var/folders/hx/6vltr8z949358k44l1m6_b7m0000gn/T/ipykernel_1059/1750840877.py:70: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[col_name] = df[\"purpose\"].str.lower().str.contains(pattern, regex=True).astype(int)\n",
      "/var/folders/hx/6vltr8z949358k44l1m6_b7m0000gn/T/ipykernel_1059/1750840877.py:70: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[col_name] = df[\"purpose\"].str.lower().str.contains(pattern, regex=True).astype(int)\n",
      "/var/folders/hx/6vltr8z949358k44l1m6_b7m0000gn/T/ipykernel_1059/1750840877.py:70: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[col_name] = df[\"purpose\"].str.lower().str.contains(pattern, regex=True).astype(int)\n",
      "/var/folders/hx/6vltr8z949358k44l1m6_b7m0000gn/T/ipykernel_1059/1750840877.py:70: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[col_name] = df[\"purpose\"].str.lower().str.contains(pattern, regex=True).astype(int)\n",
      "/var/folders/hx/6vltr8z949358k44l1m6_b7m0000gn/T/ipykernel_1059/1750840877.py:70: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[col_name] = df[\"purpose\"].str.lower().str.contains(pattern, regex=True).astype(int)\n",
      "/var/folders/hx/6vltr8z949358k44l1m6_b7m0000gn/T/ipykernel_1059/1750840877.py:70: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[col_name] = df[\"purpose\"].str.lower().str.contains(pattern, regex=True).astype(int)\n",
      "/var/folders/hx/6vltr8z949358k44l1m6_b7m0000gn/T/ipykernel_1059/1750840877.py:70: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[col_name] = df[\"purpose\"].str.lower().str.contains(pattern, regex=True).astype(int)\n",
      "Анализ пакетов: 100%|██████████████████████████| 5/5 [02:59<00:00, 35.85s/пакет]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Анализ завершён. Результаты сохранены в: data/statement_main_20_with_ai-PRO.xlsx\n",
      "Колонка risk_label окрашена и имеет фильтрацию в Excel.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "from tqdm import tqdm \n",
    "\n",
    "excel_file = \"data/statement_main_5.csv\"  \n",
    "records = read_data_from_file.func(excel_file)  # считываем и преобразуем данные\n",
    "records = detect_anomalous_transactions.func(records) # обнаружение аномальных транзакций по сумме, назначению и регулярности\n",
    "records = detect_regular_payments.func(records)\n",
    "money_flow = trace_money_flow.func(records)\n",
    "records = preprocess_data.func(records) \n",
    "records = local_model_analysis.func(records)  # локальный анализ\n",
    "\n",
    "results = []\n",
    "\n",
    "if isinstance(records, str):\n",
    "    records = json.loads(records)\n",
    "fields_to_remove = [\n",
    "    'risk_encoded', 'risk', 'risk_comment', 'date',\n",
    "    'debit_account', 'debit_name', 'debit_inn',\n",
    "    'credit_account', 'credit_name', 'credit_inn',\n",
    "    'debit_name_type_encoded', 'credit_name_type', 'credit_name_type_encoded',\n",
    "    'loan_related', 'unclear_transfer', 'no_contract', 'crypto_activity',\n",
    "    'foreign_payment', 'related_party_transfer', 'cash_out', 'donation',\n",
    "    'agent_fee', 'service_payment', 'bonus_related', 'advance_payment',\n",
    "    'lease_payment', 'logistics', 'has_docs', 'salary_related', 'utilities',\n",
    "    'tax_payment', 'supplier_payment', 'internal_transfer'\n",
    "]\n",
    "\n",
    "# fields_to_remove = [\n",
    "#     'risk_encoded', \n",
    "#     'debit_name_type_encoded', 'credit_name_type', 'credit_name_type_encoded',\n",
    "#     'loan_related', 'unclear_transfer', 'no_contract', 'crypto_activity',\n",
    "#     'foreign_payment', 'related_party_transfer', 'cash_out', 'donation',\n",
    "#     'agent_fee', 'service_payment', 'bonus_related', 'advance_payment',\n",
    "#     'lease_payment', 'logistics', 'has_docs', 'salary_related', 'utilities',\n",
    "#     'tax_payment', 'supplier_payment', 'internal_transfer'\n",
    "# ]\n",
    "\n",
    "for rec in records:\n",
    "    for f in fields_to_remove:\n",
    "        rec.pop(f, None)\n",
    "records\n",
    "\n",
    "# Пакетная отправка данных в LLM\n",
    "batch_size = 10\n",
    "results = []\n",
    "\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "def safe_parse_json(text: str):\n",
    "    \"\"\"\n",
    "    Безопасно извлекает JSON массив или объект из ответа LLM,\n",
    "    даже если перед ним есть текст, Markdown или обрезан JSON.\n",
    "    Возвращает список объектов.\n",
    "    \"\"\"\n",
    "    # Убираем блоки markdown ```json и ```\n",
    "    clean_text = re.sub(r\"^```json\\s*|```$\", \"\", text.strip(), flags=re.MULTILINE).strip()\n",
    "\n",
    "    # Пытаемся найти первый массив или объект\n",
    "    start_array = clean_text.find('[')\n",
    "    start_object = clean_text.find('{')\n",
    "\n",
    "    if start_array != -1 and (start_array < start_object or start_object == -1):\n",
    "        json_text = clean_text[start_array:]\n",
    "    elif start_object != -1:\n",
    "        json_text = clean_text[start_object:]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "    # Пытаемся парсить постепенно, если JSON обрезан\n",
    "    for i in range(len(json_text), 0, -1):\n",
    "        try:\n",
    "            parsed = json.loads(json_text[:i])\n",
    "            # Всегда возвращаем список объектов\n",
    "            if isinstance(parsed, dict):\n",
    "                return [parsed]\n",
    "            elif isinstance(parsed, list):\n",
    "                return parsed\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"Не удалось распарсить JSON\")\n",
    "    return []\n",
    "\n",
    "def parse_llm_transactions(text: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Извлекает список транзакций из JSON-ответа LLM (в том числе с обрамлением ```json ... ```).\n",
    "    Возвращает DataFrame с полями по каждой транзакции.\n",
    "    \"\"\"\n",
    "    # Убираем Markdown-обрамление\n",
    "    text = re.sub(r\"^```json\\s*\", \"\", text.strip())\n",
    "    text = re.sub(r\"\\s*```$\", \"\", text.strip())\n",
    "\n",
    "    # Убираем лишний текст до начала JSON\n",
    "    start_idx = min(\n",
    "        (text.find('{') if '{' in text else len(text)),\n",
    "        (text.find('[') if '[' in text else len(text))\n",
    "    )\n",
    "    if start_idx > 0 and start_idx < len(text):\n",
    "        text = text[start_idx:]\n",
    "\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "        # Если словарь с ключом \"transactions\"\n",
    "        if isinstance(data, dict) and 'transactions' in data:\n",
    "            transactions = data['transactions']\n",
    "        elif isinstance(data, list):\n",
    "            transactions = data\n",
    "        else:\n",
    "            transactions = []\n",
    "\n",
    "        # Преобразуем в DataFrame\n",
    "        \n",
    "        return transactions\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Ошибка JSON: {e}\")\n",
    "        return\n",
    "\n",
    "# === Основной цикл по пакетам ===\n",
    "for i in tqdm(range(0, len(records), batch_size), desc=\"Анализ пакетов\", unit=\"пакет\"):\n",
    "    batch = records[i:i+batch_size]\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    Проанализируй следующие транзакции и движения денег:\n",
    "    INPUT_DATA  = {json.dumps(batch, ensure_ascii=False)}\n",
    "    CHAINS_DATA  = {json.dumps(money_flow, ensure_ascii=False)}\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=user_prompt)\n",
    "    ])\n",
    "\n",
    "    text = response.content if hasattr(response, \"content\") else response[0].content\n",
    "\n",
    "    # Безопасный парсинг JSON\n",
    "    batch_results = safe_parse_json(text)\n",
    "    batch_results = parse_llm_transactions(text)\n",
    "    # Если batch_results — список списков, \"разворачиваем\"\n",
    "    for item in batch_results:\n",
    "        if isinstance(item, list):\n",
    "            results.extend(item)\n",
    "        elif isinstance(item, dict):\n",
    "            results.append(item)\n",
    "\n",
    "# === Сохраняем результаты в Excel ===\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Если нужно убрать колонку related_transactions\n",
    "if \"related_transactions\" in df_results.columns:\n",
    "    df_results = df_results.drop(columns=[\"related_transactions\"])\n",
    "\n",
    "output_path = \"data/statement_main_20_with_ai-PRO.xlsx\"\n",
    "df_results.to_excel(output_path, index=False)\n",
    "\n",
    "# === Цветовая маркировка и автофильтр ===\n",
    "wb = load_workbook(output_path)\n",
    "ws = wb.active\n",
    "\n",
    "# Автофильтр\n",
    "ws.auto_filter.ref = ws.dimensions\n",
    "\n",
    "# Индекс колонки risk_label\n",
    "risk_col_idx = None\n",
    "for i, col in enumerate(ws[1], start=1):\n",
    "    if col.value and col.value.lower() in [\"risk_label\", \"risk_label_ml\", \"risk_level\"]:\n",
    "        risk_col_idx = i\n",
    "        break\n",
    "\n",
    "# Цветовая карта (русские метки)\n",
    "color_map = {\n",
    "    \"зеленый\": \"90EE90\",\n",
    "    \"желтый\": \"FFFF99\",\n",
    "    \"красный\": \"FF9999\",\n",
    "}\n",
    "\n",
    "# Применяем цвета\n",
    "if risk_col_idx:\n",
    "    for row in ws.iter_rows(min_row=2, min_col=risk_col_idx, max_col=risk_col_idx):\n",
    "        cell = row[0]\n",
    "        val = str(cell.value).lower().strip()\n",
    "        if val in color_map:\n",
    "            cell.fill = PatternFill(start_color=color_map[val], end_color=color_map[val], fill_type=\"solid\")\n",
    "\n",
    "wb.save(output_path)\n",
    "wb.close()\n",
    "\n",
    "print(f\"\\nАнализ завершён. Результаты сохранены в: {output_path}\")\n",
    "print(\"Колонка risk_label окрашена и имеет фильтрацию в Excel.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e17d310-62e8-4bdc-9c35-a66cd75a0955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
