{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27b00b21",
   "metadata": {},
   "source": [
    "\n",
    "# Кластеризация транзакций: несколько моделей и сравнение метрик\n",
    "\n",
    "**Колонки, ожидаемые в данных:**  \n",
    "`date, debit_account, debit_name, debit_inn, credit_account, credit_name, credit_inn, purpose`\n",
    "\n",
    "В тетрадке:\n",
    "- Загрузка/подготовка данных\n",
    "- Построение фичей (текст, категориальные, даты)\n",
    "- Обучение нескольких моделей кластеризации: KMeans, Agglomerative, DBSCAN, GaussianMixture\n",
    "- Сравнение по метрикам: Silhouette, Calinski–Harabasz, Davies–Bouldin\n",
    "- Визуализация кластеров (t-SNE 2D)\n",
    "- Экспорт результатов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5368b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# == Импорт библиотек ==\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text as sk_text\n",
    "from sklearn.feature_extraction import _hashing as sk_hashing  # noqa: F401  # ensure availability\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "random_state = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a642dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# == Загрузка данных ==\n",
    "# Ожидается CSV с колонками из описания. По умолчанию ищем /mnt/data/transactions.csv.\n",
    "# Можно заменить путь на ваш.\n",
    "\n",
    "CSV_PATH = os.getenv(\"TX_CSV_PATH\", \"/mnt/data/transactions.csv\")\n",
    "\n",
    "def load_or_mock(csv_path):\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "    else:\n",
    "        # == Мок-данные, если реального файла нет ==\n",
    "        np.random.seed(42)\n",
    "        n = 500\n",
    "        def rand_acc(): return \"40702\" + \"\".join(np.random.choice(list(\"0123456789\"), 15))\n",
    "        purpose_pool = [\n",
    "            \"Оплата по договору поставки №123\", \"Перечисление заработной платы\",\n",
    "            \"Арендная плата за офис\", \"Возврат займа\", \"Оплата услуг связи\",\n",
    "            \"Комиссионное вознаграждение\", \"Оплата налога на прибыль\",\n",
    "            \"Транспортные услуги\", \"Материалы и комплектующие\", \"Оплата услуг ИТ-поддержки\"\n",
    "        ]\n",
    "        names = [\"ООО Альфа\", \"ООО Бета\", \"ООО Гамма\", \"АО Дельта\", \"ИП Иванов\", \"ООО Омега\"]\n",
    "        inns = [7701000010, 7702000020, 7703000030, 7804000040, 7715000050, 7726000060]\n",
    "        dates = pd.date_range(\"2024-01-01\", periods=n, freq=\"D\")\n",
    "        df = pd.DataFrame({\n",
    "            \"date\": np.random.choice(dates, n),\n",
    "            \"debit_account\": [rand_acc() for _ in range(n)],\n",
    "            \"debit_name\": np.random.choice(names, n),\n",
    "            \"debit_inn\": np.random.choice(inns, n),\n",
    "            \"credit_account\": [rand_acc() for _ in range(n)],\n",
    "            \"credit_name\": np.random.choice(names, n),\n",
    "            \"credit_inn\": np.random.choice(inns, n),\n",
    "            \"purpose\": np.random.choice(purpose_pool, n)\n",
    "        })\n",
    "        df = df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df = load_or_mock(CSV_PATH)\n",
    "print(\"Форма данных:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3106cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# == Вспомогательные функции для подготовки признаков ==\n",
    "\n",
    "def _clean_text(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(str).str.lower()\n",
    "    s = s.str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    s = s.str.replace(r\"[\\n\\r\\t]+\", \" \", regex=True)\n",
    "    return s.str.strip()\n",
    "\n",
    "def _extract_date_features(s: pd.Series) -> pd.DataFrame:\n",
    "    s = pd.to_datetime(s, errors=\"coerce\", utc=False)\n",
    "    out = pd.DataFrame({\n",
    "        \"date_year\": s.dt.year,\n",
    "        \"date_month\": s.dt.month,\n",
    "        \"date_day\": s.dt.day,\n",
    "        \"date_dow\": s.dt.dayofweek,\n",
    "        \"date_week\": s.dt.isocalendar().week.astype(int),\n",
    "        \"date_quarter\": s.dt.quarter,\n",
    "    })\n",
    "    out = out.fillna(0).astype(int)\n",
    "    return out\n",
    "\n",
    "def _as_str(s: pd.Series) -> pd.Series:\n",
    "    return s.astype(str).fillna(\"\")\n",
    "\n",
    "# Обернем в FunctionTransformer для ColumnTransformer\n",
    "date_ft = FunctionTransformer(lambda x: _extract_date_features(pd.Series(x[:,0])), validate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cc8e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# == Построение пайплайна признаков ==\n",
    "\n",
    "text_stop_words = sk_text.ENGLISH_STOP_WORDS  # добавим англ. стоп-слова; при желании можно расширить русскими\n",
    "\n",
    "# Текстовые столбцы: purpose + имена контрагентов\n",
    "text_cols = [\"purpose\", \"debit_name\", \"credit_name\"]\n",
    "\n",
    "# Счет/ИНН — как идентификаторы: хэш-векторизация по символам, чтобы не плодить миллион фиктивных категорий\n",
    "hash_cols = [\"debit_account\", \"credit_account\", \"debit_inn\", \"credit_inn\"]\n",
    "\n",
    "# Дата\n",
    "date_col = [\"date\"]\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    # Текст: TF-IDF (char+word)\n",
    "    (\"tfidf_purpose\", TfidfVectorizer(\n",
    "        preprocessor=None,\n",
    "        tokenizer=None,\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,2),\n",
    "        min_df=2,\n",
    "        max_features=30000\n",
    "    ), \"purpose\"),\n",
    "    (\"tfidf_debit_name\", TfidfVectorizer(\n",
    "        preprocessor=None,\n",
    "        tokenizer=None,\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,2),\n",
    "        min_df=2,\n",
    "        max_features=20000\n",
    "    ), \"debit_name\"),\n",
    "    (\"tfidf_credit_name\", TfidfVectorizer(\n",
    "        preprocessor=None,\n",
    "        tokenizer=None,\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,2),\n",
    "        min_df=2,\n",
    "        max_features=20000\n",
    "    ), \"credit_name\"),\n",
    "    # Идентификаторы: HashingVectorizer по символам\n",
    "    (\"hash_debit_acc\", HashingVectorizer(\n",
    "        n_features=2**12, alternate_sign=False, analyzer=\"char\", ngram_range=(4,6)\n",
    "    ), \"debit_account\"),\n",
    "    (\"hash_credit_acc\", HashingVectorizer(\n",
    "        n_features=2**12, alternate_sign=False, analyzer=\"char\", ngram_range=(4,6)\n",
    "    ), \"credit_account\"),\n",
    "    (\"hash_debit_inn\", HashingVectorizer(\n",
    "        n_features=2**10, alternate_sign=False, analyzer=\"char\", ngram_range=(2,6)\n",
    "    ), \"debit_inn\"),\n",
    "    (\"hash_credit_inn\", HashingVectorizer(\n",
    "        n_features=2**10, alternate_sign=False, analyzer=\"char\", ngram_range=(2,6)\n",
    "    ), \"credit_inn\"),\n",
    "    # Дата -> числовые признаки\n",
    "    (\"date_feats\", date_ft, [\"date\"]),\n",
    "], remainder=\"drop\", verbose_feature_names_out=False)\n",
    "\n",
    "# Основной фичепайплайн: трансформер -> SVD для сжатия\n",
    "feature_pipeline = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"svd\", TruncatedSVD(n_components=50, random_state=42)),\n",
    "    (\"scale\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Подготовим также 2D-эмбеддинг для визуализации (t-SNE сверху над SVD 50d)\n",
    "viz_pipeline = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"svd\", TruncatedSVD(n_components=50, random_state=42)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fcaa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# == Построение матрицы признаков ==\n",
    "work_df = df[[\n",
    "    \"date\",\"debit_account\",\"debit_name\",\"debit_inn\",\n",
    "    \"credit_account\",\"credit_name\",\"credit_inn\",\"purpose\"\n",
    "]].copy()\n",
    "\n",
    "# Очистка текстовых колонок для более устойчивого TF-IDF\n",
    "for col in [\"purpose\",\"debit_name\",\"credit_name\",\"debit_account\",\"credit_account\",\"debit_inn\",\"credit_inn\"]:\n",
    "    work_df[col] = _clean_text(_as_str(work_df[col]))\n",
    "\n",
    "X = feature_pipeline.fit_transform(work_df)\n",
    "print(\"Размерность признаков после SVD:\", X.shape)\n",
    "\n",
    "# 2D для визуализации\n",
    "X_2d_input = viz_pipeline.fit_transform(work_df)\n",
    "tsne = TSNE(n_components=2, random_state=random_state, init=\"pca\", learning_rate=\"auto\", perplexity=30)\n",
    "X_2d = tsne.fit_transform(X_2d_input)\n",
    "print(\"Готово 2D-представление для графиков:\", X_2d.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7af86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# == Обертки для запуска моделей и метрик ==\n",
    "\n",
    "def safe_metrics(X, labels):\n",
    "    # Модель может отдать -1 (шум) или 1 кластер => некоторые метрики не определены\n",
    "    uniq = np.unique(labels[labels != -1]) if isinstance(labels, np.ndarray) else np.unique(labels)\n",
    "    n_eff = len(uniq) if -1 in labels else len(np.unique(labels))\n",
    "    metrics = {\"n_clusters\": n_eff}\n",
    "    try:\n",
    "        if n_eff >= 2:\n",
    "            metrics[\"silhouette\"] = float(silhouette_score(X, labels, metric=\"euclidean\"))\n",
    "            metrics[\"calinski_harabasz\"] = float(calinski_harabasz_score(X, labels))\n",
    "            metrics[\"davies_bouldin\"] = float(davies_bouldin_score(X, labels))\n",
    "        else:\n",
    "            metrics[\"silhouette\"] = np.nan\n",
    "            metrics[\"calinski_harabasz\"] = np.nan\n",
    "            metrics[\"davies_bouldin\"] = np.nan\n",
    "    except Exception as e:\n",
    "        metrics[\"silhouette\"] = np.nan\n",
    "        metrics[\"calinski_harabasz\"] = np.nan\n",
    "        metrics[\"davies_bouldin\"] = np.nan\n",
    "    return metrics\n",
    "\n",
    "def run_kmeans(X, k_list=(4,6,8,10)):\n",
    "    out = []\n",
    "    for k in k_list:\n",
    "        km = KMeans(n_clusters=k, random_state=random_state, n_init=\"auto\")\n",
    "        labels = km.fit_predict(X)\n",
    "        m = safe_metrics(X, labels)\n",
    "        m.update(model=\"KMeans\", params={\"n_clusters\": k})\n",
    "        out.append((labels, m))\n",
    "    return out\n",
    "\n",
    "def run_agglomerative(X, k_list=(4,6,8,10), linkage=\"ward\"):\n",
    "    out = []\n",
    "    for k in k_list:\n",
    "        agg = AgglomerativeClustering(n_clusters=k, linkage=linkage)\n",
    "        labels = agg.fit_predict(X)\n",
    "        m = safe_metrics(X, labels)\n",
    "        m.update(model=f\"Agglomerative({linkage})\", params={\"n_clusters\": k})\n",
    "        out.append((labels, m))\n",
    "    return out\n",
    "\n",
    "def run_dbscan(X, eps_list=(0.5, 0.7, 1.0), min_samples_list=(5, 10)):\n",
    "    out = []\n",
    "    for eps in eps_list:\n",
    "        for ms in min_samples_list:\n",
    "            db = DBSCAN(eps=eps, min_samples=ms, n_jobs=-1)\n",
    "            labels = db.fit_predict(X)\n",
    "            m = safe_metrics(X, labels)\n",
    "            m.update(model=\"DBSCAN\", params={\"eps\": eps, \"min_samples\": ms})\n",
    "            out.append((labels, m))\n",
    "    return out\n",
    "\n",
    "def run_gmm(X, k_list=(4,6,8)):\n",
    "    out = []\n",
    "    for k in k_list:\n",
    "        gm = GaussianMixture(n_components=k, covariance_type=\"full\", random_state=random_state)\n",
    "        labels = gm.fit_predict(X)\n",
    "        m = safe_metrics(X, labels)\n",
    "        m.update(model=\"GaussianMixture\", params={\"n_components\": k})\n",
    "        out.append((labels, m))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3520808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# == Запуск моделей ==\n",
    "results = []\n",
    "\n",
    "models_out = []\n",
    "models_out += run_kmeans(X, k_list=(4,6,8,10))\n",
    "models_out += run_agglomerative(X, k_list=(4,6,8,10), linkage=\"ward\")\n",
    "models_out += run_agglomerative(X, k_list=(4,6,8,10), linkage=\"average\")\n",
    "models_out += run_dbscan(X, eps_list=(0.5, 0.7, 1.0), min_samples_list=(5, 10))\n",
    "models_out += run_gmm(X, k_list=(4,6,8))\n",
    "\n",
    "# Сохраним лейблы отдельно для графиков\n",
    "labels_list = []\n",
    "for labels, m in models_out:\n",
    "    labels_list.append(labels)\n",
    "    results.append(m)\n",
    "\n",
    "metrics_df = pd.DataFrame(results)\n",
    "metrics_df[\"model_id\"] = np.arange(len(metrics_df))\n",
    "metrics_df = metrics_df[[\"model_id\",\"model\",\"params\",\"n_clusters\",\"silhouette\",\"calinski_harabasz\",\"davies_bouldin\"]]\n",
    "metrics_df.sort_values(by=[\"silhouette\"], ascending=False, inplace=True)\n",
    "metrics_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "metrics_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4f3fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# == Визуализации кластеров ==\n",
    "# Для топ-5 моделей по силуэту построим 2D-графики t-SNE\n",
    "\n",
    "topn = 5\n",
    "top_ids = metrics_df.head(topn)[\"model_id\"].tolist()\n",
    "\n",
    "for mid in top_ids:\n",
    "    labels = labels_list[mid]\n",
    "    fig = plt.figure(figsize=(7, 5))\n",
    "    plt.scatter(X_2d[:,0], X_2d[:,1], c=labels, s=8)\n",
    "    row = metrics_df.loc[metrics_df[\"model_id\"]==mid].iloc[0]\n",
    "    ttl = f\"{row['model']} {row['params']} | \" \\          f\"clusters={int(row['n_clusters'])} | silhouette={row['silhouette']:.3f}\"\n",
    "    plt.title(ttl)\n",
    "    plt.xlabel(\"t-SNE 1\")\n",
    "    plt.ylabel(\"t-SNE 2\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d064a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# == Простейшие описатели кластеров по 'purpose' ==\n",
    "# Возьмем лучшую модель по силуэту и посмотрим частотность слов в 'purpose' по кластерам.\n",
    "\n",
    "best_id = metrics_df.iloc[0][\"model_id\"]\n",
    "best_labels = labels_list[int(best_id)]\n",
    "tmp = work_df.copy()\n",
    "tmp[\"cluster\"] = best_labels\n",
    "\n",
    "def top_terms_per_cluster(df, text_col=\"purpose\", topk=10):\n",
    "    from collections import Counter\n",
    "    rows = []\n",
    "    word_re = re.compile(r\"[\\w\\d\\-]+\", re.U)\n",
    "    for c, part in df.groupby(\"cluster\"):\n",
    "        tokens = []\n",
    "        for txt in part[text_col].astype(str).tolist():\n",
    "            tokens.extend(w.lower() for w in word_re.findall(txt))\n",
    "        cnt = Counter(tokens)\n",
    "        top = cnt.most_common(topk)\n",
    "        rows.append({\n",
    "            \"cluster\": c,\n",
    "            \"size\": len(part),\n",
    "            \"top_terms\": \", \".join([f\"{w}({n})\" for w, n in top])\n",
    "        })\n",
    "    return pd.DataFrame(rows).sort_values(\"size\", ascending=False)\n",
    "\n",
    "cluster_summary = top_terms_per_cluster(tmp, text_col=\"purpose\", topk=10)\n",
    "cluster_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969f7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# == Экспорт результатов ==\n",
    "OUT_DIR = \"/mnt/data\"\n",
    "metrics_path = os.path.join(OUT_DIR, \"clustering_metrics.csv\")\n",
    "labels_path = os.path.join(OUT_DIR, \"clustering_labels_best.csv\")\n",
    "\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "pd.DataFrame({\"label\": best_labels}).to_csv(labels_path, index=False)\n",
    "\n",
    "print(\"Сохранено:\")\n",
    "print(metrics_path)\n",
    "print(labels_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5b90f4",
   "metadata": {},
   "source": [
    "\n",
    "## Как запустить на ваших данных\n",
    "\n",
    "1. Подготовьте CSV с колонками: `date, debit_account, debit_name, debit_inn, credit_account, credit_name, credit_inn, purpose`  \n",
    "2. Положите файл, например, в `/mnt/data/transactions.csv` (или задайте переменную окружения `TX_CSV_PATH`).  \n",
    "3. Запустите все ячейки сверху вниз.  \n",
    "4. Смотрите таблицу метрик, графики и экспорт в `/mnt/data/`.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
